---
title: "Regression: predictive modelling"
subtitle: "ENVX2001 - Applied Statistical Methods"
date: today
date-format: "MMM YYYY"
author: 
  - name: Januar Harianto
    affiliation: School of Life and Envoronmental Sciences
institute: The University of Sydney
format:
  revealjs: 
    theme: [default, theme.scss]
    slide-number: c/t
    code-copy: true
    code-link: false
    code-overflow: wrap
    highlight-style: arrow
    html-math-method: katex
    embed-resources: false # set to true once complete
execute: 
  eval: true
  echo: true
  freeze: auto  # re-render only when source changes
editor_options: 
  chunk_output_type: console
  render-on-save: true  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  cache = TRUE)
library(tidyverse)
ggplot2::theme_set(cowplot::theme_half_open())
# ggplot2::theme_set(ggplot2::theme_minimal())
```




# Predictive modelling

> "The best way to predict the future is to create it."

-- Peter Ferdinand Drucker, 1909--2005


# Our workflow so far

## Workflow {auto-animate="true"}

1. Model development -- week 7
   - **Explore**: visualise, summarise
   - **Transform predictors**: linearise, reduce skewness/leverage
   - **Model**: fit, check assumptions, interpret, transform. Repeat.

. . .

2. Variable selection -- week 8
   - **VIF**: remove predictors with high variance inflation factor
   - **Model selection**: stepwise selection, AIC, principle of parsimony, assumption checks

. . .

3. **This week -- predictive modelling**
   - **Predict**: Use the model to predict new data
   - **Assess**: Evaluate the model's performance
   - **Reflect**: What can we do to improve the model?


# Making predictions

## Previously on ENVX2001... {auto-animate="true"}


We fitted a multiple linear regression model to the data (slide 62).

```{r}
#| message=FALSE, warning=FALSE
library(tidyverse)
multi_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)
summary(multi_fit)
```

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot Temp + 0.00252 \cdot Solar.R - 0.0616 \cdot Wind$$

## Predict: equation {auto-animate="true"}

$$ \widehat{log(Ozone)}=-0.262 + 0.0492 \cdot \color{darkorchid}{Temp} + 0.00252 \cdot \color{darkorange}{Solar.R} - 0.0616 \cdot \color{seagreen}{Wind} $$

. . .

On a certain day, we measured (*units are Imperial*):

- [temperature `Temp` to be 80 degrees Fahrenheit]{style="color: darkorchid"}
- [solar radiation `Solar.R` to be 145 units (Langleys)]{style="color: darkorange"}
- [wind speed `Wind` to be 10.9 miles per hour]{style="color: seagreen"}
  
**What is the predicted ozone level?**
. . .

$$\widehat{log(Ozone)}=-0.262 + 0.0492 \cdot \color{darkorchid}{80} + 0.00252 \cdot \color{darkorange}{145} - 0.0616 \cdot \color{seagreen}{10.9}$$

Easy! The two things we need to think about are...

- **What is the uncertainty in this prediction?**


## Uncertainty

. . .

- **Confidence interval**: uncertainty in the **mean** response at a given predictor value.
- **Prediction interval**: uncertainty in a **single** response at a given predictor value.

<br>

. . .

### What it means

> **95% confidence interval**: Given the parameters of the model, we are 95% confident that the *mean* response at a given predictor value is between $y_1$ and $y_2$.

> **95% prediction interval**: Given the parameters of the model, we are 95% confident that a *single* response at a given predictor value is between $y_1$ and $y_2$.


### Why the distinction?

- **Confidence interval**: we are interested in the *mean* response.
- **Prediction interval**: we are interested in a *single* value prediction.

## Confidence interval (CI)

### CI: standard error of the fit

$$ se(\widehat{y}) = \sqrt{MSE \cdot \left( \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right)} $$ where $x_0$ is the predictor value at which we want to predict the response

- $MSE$ is the mean squared error of the fit (residual ms)
- $\sum_{i=1}^n (x_i - \bar{x})^2$ is the sum of squares of the predictor values
- $n$ is the number of observations
- $\bar{x}$ is the mean of the predictor values

## Prediction interval (PI)

### PI: standard error of the prediction

$$ se(\widehat{y}) = \sqrt{MSE \cdot \left( 1 + \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right)} $$ where $x_0$ is the predictor value at which we want to predict the response

- $MSE$ is the mean squared error of the fit (residual ms)
- $\sum_{i=1}^n (x_i - \bar{x})^2$ is the sum of squares of the predictor values
- $n$ is the number of observations
- $\bar{x}$ is the mean of the predictor values


:::{.callout-note .fragment .fade-up}
The only difference between the CI and PI is the additional term $1$ in the PI formula that is added. The reason for this is that we are interested in a *single* response, not the *mean* response.
:::

## Predictions in R

- We can use the `predict()` function
- First, we need to create a new data frame with the predictor values we want to predict at

```{r}
#| message=FALSE, warning=FALSE
to_predict <- data.frame(Temp = 80, Solar.R = 145, Wind = 10.9)
```

- Then, we can use the `predict()` function to predict the response at these values
- Use `interval = "confidence"` or `interval = "prediction"` to get the confidence or prediction interval.


```{r}
#| message=FALSE, warning=FALSE
predict(multi_fit, newdata = to_predict, interval = "confidence")
predict(multi_fit, newdata = to_predict, interval = "prediction")
```

## Comparing CI vs PI

- The confidence interval is narrower than the prediction interval.
- It's easier to visualise two-dimensional data, so let's look at a simple linear regression model of `log(Ozone)` vs. `Temp`.

```{r}
fit <- lm(log(Ozone) ~ Temp, data = airquality)
```

- We create a range of predictions across all possible `Temp` values in 0.1 &deg;F increments, and calculate both the CI and PI for each of those values

```{r}
# Generate values to predict at in 0.1 degree increments
to_pred <- data.frame(Temp = seq(min(airquality$Temp), max(airquality$Temp), by = 0.1))
preds_ci <- predict(fit, newdata = to_pred, interval = "confidence") # confidence interval
preds_pi <- predict(fit, newdata = to_pred, interval = "prediction") # prediction interval
```

- Extract only upper and lower CI and PI values and merge the data frames

```{r}
pred_df <- data.frame(Temp = to_pred$Temp,
                      Lci = preds_ci[, "lwr"],
                      Uci = preds_ci[, "upr"],
                      Lpi = preds_pi[, "lwr"],
                      Upi = preds_pi[, "upr"])
```

## Visualising CI vs PI

- We can now plot the CI and PI as shaded areas around the predicted line

```{r}
#| code-fold: true
p <-
  ggplot(airquality, aes(Temp, log(Ozone))) +
  geom_point() + 
  geom_line(data = pred_df, aes(Temp, Lci), color = "blue") +
  geom_line(data = pred_df, aes(Temp, Uci), color = "blue") +
  geom_line(data = pred_df, aes(Temp, Lpi), color = "red") +
  geom_line(data = pred_df, aes(Temp, Upi), color = "red") +
  labs(x = "Temperature (F)", y = "log(Ozone)") +
  theme_bw()
p
```

## CI and `geom_smooth()`

- Notice that `geom_smooth()` uses the CI, not the PI.

```{r}
#| code-fold: true
p + geom_smooth(method = "lm", se = TRUE)
```

## Limitations

All is good when we want to assess uncertainty in a model that we have already fit. But what if we want to know how well the model predicts **new** data, i.e. data that we did not use to fit the model?

### What we need

- A way to estimate how well the model predicts new data *that hasn't been used to fit the model*, i.e. an **independent** dataset.
- Because we have the **actual values** in the new dataset, we can compare them to the **predicted values** from the model.
  - If the model is good, we expect the predictions to be close to the actual values.
  - If the model is bad, we expect the predictions to be *far* from the actual values.

# Model validation

## General idea

- We have a dataset that we use to fit a model, and want to assess how well the model predicts new data by performing **model validation**.
- We can:
  - ~~use the *same* dataset to assess how well the model fits the data (e.g. $r^2$);~~ or
  - **use a *different* dataset to assess how well the model predicts new data (e.g. RMSE).**
- The dataset can be obtained by:
  - Collecting new data.
  - Splitting the existing data into two parts before model building.
  - Cross-validation or k-fold cross-validation of existing data.

## Collecting new data {auto-animate=true}

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 500px; height: 50px; margin: 10px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=275 data-id="text1"}

## Collecting new data {auto-animate=true}
::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 500px; height: 50px; margin: 10px;"}
:::

$+$

::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 200px; height: 50px; margin: 10px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=155 data-id="text1"}

[New dataset]{style="color: #3fb618;" .absolute top=130 left=700}



## Collecting new data

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 500px; height: 50px; margin: 10px;"}
:::

$+$

::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 200px; height: 50px; margin: 10px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=155 data-id="text1"}

[New dataset]{style="color: #3fb618;" .absolute top=130 left=700}

<br>
<br>

- The best way to assess how well a model predicts new data is to collect new data.
  - [**Training set**]{style="color: #2780e3"}: used to fit the model.
  - [**Test set**]{style="color: #3fb618"}: used to assess how well the model predicts new data.


### Pros

- The new data is completely independent of the data used to fit the model.
- Compared to splitting existing data, we have *more* data to fit the model and *more* data to validate the model.

### Cons

- It can be expensive and time-consuming to collect new data.
- Some data may be impossible to collect (e.g. historical data).


## Data splitting {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 500px; height: 50px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 0px; height: 50px; margin: 0px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=275 data-id="text1"}

## Data splitting {auto-animate=true auto-animate-easing="ease-in-out"}

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 400px; height: 50px; margin: 0px;"}
:::

::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 50px; margin: 0px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=275 data-id="text1"}

[(Training)]{style="color: #2780e3;" .absolute top=130 left=368 data-id="text2"}

[Subset (Test)]{style="color: #3fb618;" .absolute top=130 left=680}

## Data splitting

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 400px; height: 50px; margin: 0px;"}
:::

::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 50px; margin: 0px;"}
:::
:::

[Dataset]{style="color: #2780e3;" .absolute top=130 left=275 data-id="text1"}

[(Training)]{style="color: #2780e3;" .absolute top=130 left=368 data-id="text2"}

[Subset (Test)]{style="color: #3fb618;" .absolute top=130 left=680}

<br>
<br>

- Split the existing data into two parts:
  - [**Training set**]{style="color: #2780e3"}: used to fit the model.
  - [**Test set**]{style="color: #3fb618"}: used to assess how well the model predicts new data.

### Pros

- The test set is completely independent of the training set.
- Compared to collecting new data, it is cheaper and faster to split existing data.

### Cons

- We have *less* data to fit the model and *less* data to validate the model.
- How do we split the data?


## Cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Single dataset -->

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 500px; height: 50px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 0px; height: 50px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 0px; height: 50px; margin: 0px;"}
:::
:::

## Cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Cross-validated dataset (random split) -->

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 50px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 50px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 50px; margin: 0px;"}
:::
:::

## Cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Make bars smaller -->

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 20px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 20px; margin: 0px;"}
:::
:::

## Cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Add iterations -->

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 20px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 20px; margin: 0px;"}
:::
:::

::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 20px; margin: 0px;"}
:::
:::


::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 250px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 150px; height: 20px; margin: 0px;"}
:::
:::

[Iteration 1]{style="color: #000; font-size: 18px;" .absolute top=55 right=790 data-id="text1"}

[Iteration 2]{style="color: #000; font-size: 18px;" .absolute top=75 right=790 data-id="text2"}

[Iteration 3]{style="color: #000; font-size: 18px;" .absolute top=95 right=790 data-id="text3"}

[And so on...]{style="color: #000; font-size: 18px;" .absolute top=115 right=790 data-id="text4"}


## Cross-validation

::: {.r-hstack}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 20px; margin: 0px;"}
:::
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 20px; margin: 0px;"}
:::
:::

::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 300px; height: 20px; margin: 0px;"}
:::
:::


::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 250px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 100px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 150px; height: 20px; margin: 0px;"}
:::
:::

[Iteration 1]{style="color: #000; font-size: 18px;" .absolute top=55 right=790 data-id="text1"}

[Iteration 2]{style="color: #000; font-size: 18px;" .absolute top=75 right=790 data-id="text2"}

[Iteration 3]{style="color: #000; font-size: 18px;" .absolute top=95 right=790 data-id="text3"}

[And so on...]{style="color: #000; font-size: 18px;" .absolute top=115 right=790 data-id="text4"}

<br>

- Like data splitting, where existing data is split into two parts:
  - [**Training set**]{style="color: #2780e3"}: used to fit the model.
  - [**Test set**]{style="color: #3fb618"}: used to assess how well the model predicts new data.
- The **difference** is that the splitting is done *multiple* times, and the model is fit and validated *multiple* times.

### Pros

- Same as data splitting, but also:
  - The model is fit and validated *multiple* times, so we can get a better estimate of how well the model predicts new data.

### Cons

- We have *less* data to fit the model and *less* data to validate the model.
- It can be computationally expensive to perform cross-validation. 


## *k*-fold cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Single dataset -->

::: {.r-hstack}
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 0px; height: 50px; margin: 0px;"}
:::
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 498px; height: 50px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 0px; height: 50px; margin: 0px;"}
:::
:::

## *k*-fold cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Fold 1 -->

::: {.r-hstack}
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 50px; margin: 0px;"}
:::
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 50px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 50px; margin: 0px;"}
:::
:::

## *k*-fold cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Make bars smaller -->

::: {.r-hstack}
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
:::

## *k*-fold cross-validation {auto-animate=true auto-animate-easing="ease-in-out"}
<!-- Add iterations -->

::: {.r-hstack}
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
:::

::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
:::


::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
:::

[Iteration 1]{style="color: #000; font-size: 18px;" .absolute top=55 right=790 data-id="text1"}
[Iteration 2]{style="color: #000; font-size: 18px;" .absolute top=75 right=790 data-id="text2"}
[Iteration 3]{style="color: #000; font-size: 18px;" .absolute top=95 right=790 data-id="text3"}
[3-fold cross-validation]{style="color: #000; font-size: 18px;" .absolute top=120 right=790 data-id="text4"}
[Fold 1]{style="color: #000; font-size: 18px;" .absolute top=120 right=660}
[Fold 2]{style="color: #000; font-size: 18px;" .absolute top=120 right=494}
[Fold 3]{style="color: #000; font-size: 18px;" .absolute top=120 right=328}



## *k*-fold cross-validation

::: {.r-hstack}
::: {data-id="box2" auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {data-id="box3" auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
:::

::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
:::


::: {.r-hstack}
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #2780e3; width: 166px; height: 20px; margin: 0px;"}
:::
::: {auto-animate-delay="0" style="background: #3fb618; width: 166px; height: 20px; margin: 0px;"}
:::
:::

[Iteration 1]{style="color: #000; font-size: 18px;" .absolute top=55 right=790 data-id="text1"}
[Iteration 2]{style="color: #000; font-size: 18px;" .absolute top=75 right=790 data-id="text2"}
[Iteration 3]{style="color: #000; font-size: 18px;" .absolute top=95 right=790 data-id="text3"}
[3-fold cross-validation]{style="color: #000; font-size: 18px;" .absolute top=120 right=790 data-id="text4"}
[Fold 1]{style="color: #000; font-size: 18px;" .absolute top=120 right=660}
[Fold 2]{style="color: #000; font-size: 18px;" .absolute top=120 right=494}
[Fold 3]{style="color: #000; font-size: 18px;" .absolute top=120 right=328}

<br>

- split data into *k* groups (folds)
- [**Train**]{style="color: #2780e3"} on *k-1* folds, [**Test**]{style="color: #3fb618"} on the remaining fold
- All folds are used for testing once

### Pros

- Same as cross-validation, but also:
  - Better use of *all* available data
  - Greatly reduces overfitting as the model's performance is not just a result of the particular way the data was split.

### Cons

- Computationally expensive, since all data is used for training and testing
- Bias in small datasets: each fold may contain too little data to provide a representative sample 

# Assessing prediction quality

## Root-mean-square error (RMSE)

The **most common metric** for comparing the performance of regression models.
$$ RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2} $$

> Approximately, the standard deviation of the residuals.

- A measure of **accuracy** for the model.
- Unlike $r^2$, RMSE is **not** a relative measure, but is scaled to the units of $y$.
- The smaller the RMSE, the better the model.

## Mean error (ME)

The average of the residuals.

$$ ME = \frac{1}{n}\sum_{i=1}^{n}y_i - \hat{y}_i $$

> Averaged difference between the predicted and observed values.

- A measure of **bias** for the model.
- Also scaled to the units of $y$.
- Can be positive or negative to indicate over- or under-estimation.


## Lin's concordance correlation coefficient (CCC)

A modification of Pearson's correlation coefficient that takes into account the deviation of the observations from the identity line (i.e., the 45&deg; line where the values of the two variables are equal).

$$ CCC = \frac{2\text{Cov}(X,Y)}{\text{Var}(X) + \text{Var}(Y) + (\mu_X - \mu_Y)^2} $$

> An "agreement" value that takes into account covariance, variances, and difference in means.


- A measure of **precision** for the model.
- Ranges from -1 to 1, with 1 indicating perfect agreement.
- Unitless and scale-invariant.

## CCC vs Pearson correlation coefficient

```{r}
#| code-fold: true
library(tidyverse)

df <- tibble(y = seq(0, 100, 5),
  "45 degree line | CCC = 1" = seq(0, 100, 5)) %>%
  mutate("Location shift | CCC = 0.89" = `45 degree line | CCC = 1` - 15) %>%
  mutate("Scale shift | CCC = 0.52" = y / 2) %>%
  mutate("Location and scale shift | CCC = 0.67" = y * 2 - 20)

# pivot
df_long <- df %>%
  pivot_longer(-1, values_to = "x") %>%
  mutate(name = factor(name, 
    levels = c("45 degree line | CCC = 1",
      "Location shift | CCC = 0.89",
      "Scale shift | CCC = 0.52",
      "Location and scale shift | CCC = 0.67")))

ggplot(df_long, aes(x, y)) +
  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = "grey") +
  facet_wrap(~name) +
  geom_point() +
  xlim(0, 100) +
  labs(x = "", y = "") +
  theme_bw() +
  geom_blank() 
```

```{r}
#| include: false
# Calculate CCC
# library(DescTools)
# CCC(df$y, df$`45 degree line | CCC = 1`)$rho.c[[1]]
# CCC(df$y, df$`Location shift | CCC = 0.89`)$rho.c[[1]]
# CCC(df$y, df$`Scale shift | CCC = 0.52`)$rho.c[[1]]
# CCC(df$y, df$`Location and scale shift | CCC = 0.67`)$rho.c[[1]]

```

All of the above have a Pearson correlation coefficient of 1.

# Putting into practice

```{r}
library(caret)
```


# Thanks!

**Questions? Comments?**

Slides made with [Quarto](https://quarto.org)
