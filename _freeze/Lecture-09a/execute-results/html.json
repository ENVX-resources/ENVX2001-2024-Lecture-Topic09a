{
  "hash": "6fd8e6a9e05817481b4b46f9a9efb4b5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression: predictive modelling -- Part 1\"\nsubtitle: \"ENVX2001 - Applied Statistical Methods\"\nauthor:\n  - name: Liana Pozza\n    affiliations: The University of Sydney\ndate: last-modified\nself-contained: true\nexecute:\n  freeze: auto\n  cache: false\n# NOTE: please check _quarto.yml file for more options\n---\n\n\n\n\n\n\n\n# Predictive modelling\n\n> \"The best way to predict the future is to create it.\"\n\n-- Peter Ferdinand Drucker, 1909--2005\n\n\n# Our workflow so far\n\n## Workflow {auto-animate=\"true\"}\n\n1. Model development\n   - **Explore**: visualise, summarise\n   - **Transform predictors**: linearise, reduce skewness/leverage\n   - **Model**: fit, check assumptions, interpret, transform. Repeat.\n\n. . .\n\n2. Variable selection\n   - **VIF**: remove predictors with high variance inflation factor\n   - **Model selection**: stepwise selection, AIC, principle of parsimony, assumption checks\n\n. . .\n\n3. **Predictive modelling**\n   - **Predict**: Use the model to predict new data\n   - **Validate**: Evaluate the model's performance\n\n# Making predictions\n\n## Previously on ENVX2001... {auto-animate=\"true\"}\n\n\nWe fitted a multiple linear regression model to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmulti_fit <- lm(log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\nsummary(multi_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(Ozone) ~ Temp + Solar.R + Wind, data = airquality)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.06193 -0.29970 -0.00231  0.30756  1.23578 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.2621323  0.5535669  -0.474 0.636798    \nTemp         0.0491711  0.0060875   8.077 1.07e-12 ***\nSolar.R      0.0025152  0.0005567   4.518 1.62e-05 ***\nWind        -0.0615625  0.0157130  -3.918 0.000158 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5086 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6644,\tAdjusted R-squared:  0.655 \nF-statistic: 70.62 on 3 and 107 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n$$\\widehat{log(Ozone)}=-0.262 + 0.0492 \\cdot Temp + 0.00252 \\cdot Solar.R - 0.0616 \\cdot Wind$$\n\n## Predict: equation {auto-animate=\"true\"}\n\n$$ \\widehat{log(Ozone)}=-0.262 + \\color{darkorchid}{0.0492} \\cdot Temp + \\color{darkorange}{0.00252} \\cdot Solar.R - \\color{seagreen}{0.0616} \\cdot Wind $$\n\n. . .\n\nOn a certain day, we measured (*units are Imperial*):\n\n- [temperature `Temp` to be 80 degrees Fahrenheit]{style=\"color: darkorchid\"}\n- [solar radiation `Solar.R` to be 145 units (Langleys)]{style=\"color: darkorange\"}\n- [wind speed `Wind` to be 10.9 miles per hour]{style=\"color: seagreen\"}\n  \n**What is the predicted ozone level?**\n. . .\n\n$$\\widehat{log(Ozone)}= -0.262 + \\color{darkorchid}{0.0492 \\cdot 80} + \\color{darkorange}{0.00252 \\cdot 145} - \\color{seagreen}{0.0616 \\cdot 10.9}$$\n\nEasy! The two things we need to think about are...\n\n- **What is the uncertainty in this prediction?**\n- **Can this model be used to predict ozone if we collect new data in the future?**\n\n\n## Uncertainty\n\n. . .\n\n- **Confidence interval**: uncertainty in the **mean** response at a given predictor value.\n- **Prediction interval**: uncertainty in a **single** response at a given predictor value.\n\n. . .\n\n### What it means\n\n> **95% confidence interval**: Given the parameters of the model, we are 95% confident that the *mean* response at a given predictor value is between $y_1$ and $y_2$.\n\n> **95% prediction interval**: Given the parameters of the model, we are 95% confident that a *single* response at a given predictor value is between $y_1$ and $y_2$.\n\n\n### Why the distinction?\n\n- **Confidence interval**: we are interested in the *mean* response.\n- **Prediction interval**: we are interested in a *single* value prediction.\n\n## Confidence interval (CI)\n\n### CI: standard error of the fit\n\n$$ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)} $$ where $x_0$ is the predictor value at which we want to predict the response\n\n- $MSE$ is the mean squared error of the fit (residual ms)\n- $\\sum_{i=1}^n (x_i - \\bar{x})^2$ is the sum of squares of the predictor values\n- $n$ is the number of observations\n- $\\bar{x}$ is the mean of the predictor values\n\n## Prediction interval (PI)\n\n### PI: standard error of the prediction\n\n$$ se(\\widehat{y}) = \\sqrt{MSE \\cdot \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)} $$ where $x_0$ is the predictor value at which we want to predict the response\n\n- $MSE$ is the mean squared error of the fit (residual ms)\n- $\\sum_{i=1}^n (x_i - \\bar{x})^2$ is the sum of squares of the predictor values\n- $n$ is the number of observations\n- $\\bar{x}$ is the mean of the predictor values\n\n\nThe only difference between the CI and PI is the additional term $1$ in the PI formula that is added. The reason for this is that we are interested in a *single* response, not the *mean* response.\n\n\n## Predictions in R\n\n- We can use the `predict()` function\n- First, we need to create a new data frame with the predictor values we want to predict at\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_predict <- data.frame(Temp = 80, Solar.R = 145, Wind = 10.9)\n```\n:::\n\n\n- Then, we can use the `predict()` function to predict the response at these values\n- Use `interval = \"confidence\"` or `interval = \"prediction\"` to get the confidence or prediction interval.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(multi_fit, newdata = to_predict, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 3.365227 3.246265 3.484189\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(multi_fit, newdata = to_predict, interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 3.365227 2.350051 4.380404\n```\n\n\n:::\n:::\n\n\n## Comparing CI vs PI\n\n- The confidence interval is narrower than the prediction interval.\n- It's easier to visualise two-dimensional data, so let's look at a simple linear regression model of `log(Ozone)` vs. `Temp`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(log(Ozone) ~ Temp, data = airquality)\n```\n:::\n\n\n\n## Comparing CI vs PI\n\n- We create a range of predictions across all possible `Temp` values in 0.1 &deg;F increments, and calculate both the CI and PI for each of those values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate values to predict at in 0.1 degree increments\nto_pred <- data.frame(Temp = seq(min(airquality$Temp), max(airquality$Temp), by = 0.1))\npreds_ci <- predict(fit, newdata = to_pred, interval = \"confidence\") # confidence interval\npreds_pi <- predict(fit, newdata = to_pred, interval = \"prediction\") # prediction interval\n```\n:::\n\n\n- Extract only upper and lower CI and PI values and merge the data frames\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_df <- data.frame(Temp = to_pred$Temp,\n                      Lci = preds_ci[, \"lwr\"],\n                      Uci = preds_ci[, \"upr\"],\n                      Lpi = preds_pi[, \"lwr\"],\n                      Upi = preds_pi[, \"upr\"])\n```\n:::\n\n\n## Visualising CI vs PI\n\n- We can now plot the CI and PI as shaded areas around the predicted line\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np <-\n  ggplot(airquality, aes(Temp, log(Ozone))) +\n  geom_point() + \n  geom_line(data = pred_df, aes(Temp, Lci), color = \"blue\") +\n  geom_line(data = pred_df, aes(Temp, Uci), color = \"blue\") +\n  geom_line(data = pred_df, aes(Temp, Lpi), color = \"red\") +\n  geom_line(data = pred_df, aes(Temp, Upi), color = \"red\") +\n  labs(x = \"Temperature (F)\", y = \"log(Ozone)\") +\n  theme_bw()\np\n```\n\n::: {.cell-output-display}\n![](Lecture-09a_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n## CI and `geom_smooth()`\n\n- Notice that `geom_smooth()` uses the CI, not the PI.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\np + geom_smooth(method = \"lm\", se = TRUE)\n```\n\n::: {.cell-output-display}\n![](Lecture-09a_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n## Limitations\n\nAll is good when we want to assess uncertainty in a model that we have already fit. But what if we want to know how well the model predicts **new** data, i.e. data that we did not use to fit the model?\n\n### What we need\n\n- A way to estimate how well the model predicts new data *that hasn't been used to fit the model*, i.e. an **independent** dataset.\n- Because we have the **actual values** in the new dataset, we can compare them to the **predicted values** from the model.\n  - If the model is good, we expect the predictions to be close to the actual values.\n  - If the model is bad, we expect the predictions to be *far* from the actual values.\n\n# Model validation\n\n## General idea\n\n- We have a dataset that we use to fit a model, and want to assess how well the model predicts new data by performing **model validation**.\n- We can:\n  - ~~use the *same* dataset to assess how well the model fits the data (e.g. $r^2$);~~ or\n  - **use a *different* dataset to assess how well the model predicts new data (e.g. RMSE).**\n- The dataset can be obtained by:\n  - Collecting new data.\n  - Splitting the existing data into two parts before model building.\n  - Cross-validation or k-fold cross-validation of existing data.\n\n## Collecting new data {auto-animate=true}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 10px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n## Collecting new data {auto-animate=true}\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 10px;\"}\n:::\n\n$+$\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 200px; height: 50px; margin: 10px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=155 data-id=\"text1\"}\n\n[New dataset]{style=\"color: #95D840FF;\" .absolute top=130 left=700}\n\n\n\n## Collecting new data\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 10px;\"}\n:::\n\n$+$\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 200px; height: 50px; margin: 10px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=155 data-id=\"text1\"}\n\n[New dataset]{style=\"color: #95D840FF;\" .absolute top=130 left=700}\n\n\n- The best way to assess how well a model predicts new data is to collect new data.\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n\n\n## Collecting new data\n\n### Pros\n\n- The new data is completely independent of the data used to fit the model.\n- Compared to splitting existing data, we have *more* data to fit the model and *more* data to validate the model.\n\n### Cons\n\n- It can be expensive and time-consuming to collect new data.\n- Some data may be impossible to collect (e.g. historical data).\n\n\n## Data splitting {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n## Data splitting {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 400px; height: 50px; margin: 0px;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n[(Training)]{style=\"color: #2D708EFF;\" .absolute top=130 left=368 data-id=\"text2\"}\n\n[Subset (Test)]{style=\"color: #95D840FF;\" .absolute top=130 left=680}\n\n## Data splitting\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 400px; height: 50px; margin: 0px;\"}\n:::\n\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n[Dataset]{style=\"color: #2D708EFF;\" .absolute top=130 left=275 data-id=\"text1\"}\n\n[(Training)]{style=\"color: #2D708EFF;\" .absolute top=130 left=368 data-id=\"text2\"}\n\n[Subset (Test)]{style=\"color: #95D840FF;\" .absolute top=130 left=680}\n\n\n- Split the existing data into two parts:\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n\n\n## Data splitting\n\n### Pros\n\n- The test set is completely independent of the training set.\n- Compared to collecting new data, it is cheaper and faster to split existing data.\n\n### Cons\n\n- We have *less* data to fit the model and *less* data to validate the model.\n- How do we split the data?\n\n\n## Cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Single dataset -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 500px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## Cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Cross-validated dataset (random split) -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## Cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Make bars smaller -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n## Cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Add iterations -->\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 250px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 150px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n\n[And so on...]{style=\"color: #000; font-size: 18px;\" .absolute top=115 right=790 data-id=\"text4\"}\n\n\n## Cross-validation\n\n::: {.r-hstack}\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 300px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 250px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 100px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 150px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n\n[And so on...]{style=\"color: #000; font-size: 18px;\" .absolute top=115 right=790 data-id=\"text4\"}\n\n\n\n- Like data splitting, where existing data is split into two parts:\n  - [**Training set**]{style=\"color: #2D708EFF\"}: used to fit the model.\n  - [**Test set**]{style=\"color: #95D840FF\"}: used to assess how well the model predicts new data.\n- The **difference** is that the splitting is done *multiple* times, and the model is fit and validated *multiple* times.\n\n\n## Cross-validation\n\n### Pros\n\n- Same as data splitting, but also:\n  - The model is fit and validated *multiple* times, so we can get a better estimate of how well the model predicts new data.\n\n### Cons\n\n- We have *less* data to fit the model and *less* data to validate the model.\n- It can be computationally expensive to perform cross-validation. \n\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Single dataset -->\n\n::: {.r-hstack}\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 498px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 0px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Fold 1 -->\n\n::: {.r-hstack}\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 50px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 50px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Make bars smaller -->\n\n::: {.r-hstack}\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n## *k*-fold cross-validation {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n<!-- Add iterations -->\n\n::: {.r-hstack}\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n[3-fold cross-validation]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=790 data-id=\"text4\"}\n[Fold 1]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=660}\n[Fold 2]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=494}\n[Fold 3]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=328}\n\n\n\n## *k*-fold cross-validation\n\n::: {.r-hstack}\n::: {data-id=\"box2\" auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box1\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {data-id=\"box3\" auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n\n::: {.r-hstack}\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #2D708EFF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n::: {auto-animate-delay=\"0\" style=\"background: #95D840FF; width: 166px; height: 20px; margin: 0px;\"}\n:::\n:::\n\n[Iteration 1]{style=\"color: #000; font-size: 18px;\" .absolute top=55 right=790 data-id=\"text1\"}\n[Iteration 2]{style=\"color: #000; font-size: 18px;\" .absolute top=75 right=790 data-id=\"text2\"}\n[Iteration 3]{style=\"color: #000; font-size: 18px;\" .absolute top=95 right=790 data-id=\"text3\"}\n[3-fold cross-validation]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=790 data-id=\"text4\"}\n[Fold 1]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=660}\n[Fold 2]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=494}\n[Fold 3]{style=\"color: #000; font-size: 18px;\" .absolute top=120 right=328}\n\n\n\n- split data into *k* groups (folds)\n- [**Train**]{style=\"color: #2D708EFF\"} on *k-1* folds, [**Test**]{style=\"color: #95D840FF\"} on the remaining fold\n- All folds are used for testing once\n\n\n## *k*-fold cross-validation\n\n### Pros\n\n- Same as cross-validation, but also:\n  - Better use of *all* available data\n  - Greatly reduces overfitting as the model's performance is not just a result of the particular way the data was split.\n\n### Cons\n\n- Computationally expensive, since all data is used for training and testing\n- Bias in small datasets: each fold may contain too little data to provide a representative sample \n\n# Assessing prediction quality\n\n## Root-mean-square error (RMSE)\n\nThe **most common metric** for comparing the performance of regression models.\n$$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$\n\n> Approximately, the standard deviation of the residuals.\n\n- A measure of **accuracy** for the model.\n- Unlike $r^2$, RMSE is **not** a relative measure, but is scaled to the units of $y$.\n- The smaller the RMSE, the better the model.\n\n## Mean error (ME)\n\nThe average of the residuals.\n\n$$ ME = \\frac{1}{n}\\sum_{i=1}^{n}y_i - \\hat{y}_i $$\n\n> Averaged difference between the predicted and observed values.\n\n- A measure of **bias** for the model.\n- Also scaled to the units of $y$.\n- Can be positive or negative to indicate over- or under-estimation.\n\n\n## Lin's concordance correlation coefficient (CCC)\n\nA modification of Pearson's correlation coefficient that takes into account the deviation of the observations from the identity line (i.e., the 45&deg; line where the values of the two variables are equal).\n\n$$ CCC = \\frac{2\\text{Cov}(X,Y)}{\\text{Var}(X) + \\text{Var}(Y) + (\\mu_X - \\mu_Y)^2} $$\n\n> An \"agreement\" value that takes into account covariance, variances, and difference in means.\n\n\n- A measure of **precision** for the model.\n- Ranges from -1 to 1, with 1 indicating perfect agreement.\n- Unitless and scale-invariant.\n\n## CCC vs Pearson correlation coefficient\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(tidyverse)\n\ndf <- tibble(y = seq(0, 100, 5),\n  \"45 degree line | CCC = 1\" = seq(0, 100, 5)) %>%\n  mutate(\"Location shift | CCC = 0.89\" = `45 degree line | CCC = 1` - 15) %>%\n  mutate(\"Scale shift | CCC = 0.52\" = y / 2) %>%\n  mutate(\"Location and scale shift | CCC = 0.67\" = y * 2 - 20)\n\n# pivot\ndf_long <- df %>%\n  pivot_longer(-1, values_to = \"x\") %>%\n  mutate(name = factor(name, \n    levels = c(\"45 degree line | CCC = 1\",\n      \"Location shift | CCC = 0.89\",\n      \"Scale shift | CCC = 0.52\",\n      \"Location and scale shift | CCC = 0.67\")))\n\nggplot(df_long, aes(x, y)) +\n  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = \"grey\") +\n  facet_wrap(~name) +\n  geom_point() +\n  xlim(0, 100) +\n  labs(x = \"\", y = \"\") +\n  theme_bw() +\n  geom_blank() \n```\n\n::: {.cell-output-display}\n![](Lecture-09a_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\nAll of the above have a Pearson correlation coefficient of 1.\n\n# Next Lecture\n\nWe will go through several examples to practice data splitting, cross-validation, and model evaluation.\n\n\n# Thanks!\n\n**Questions? Comments?**\n\nSlides made with [Quarto](https://quarto.org)\n",
    "supporting": [
      "Lecture-09a_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}